{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "The idea behind Principal component anaylsis (PCA) is to take a set of data in $D$-dimensional space, and capture as much of the information (variance) in these data as possible as linear combinations of the dimensions as possible. This allows us to greatly reduce the problem space, in particular for data with many superfluous dimensions where most of the variation can be explained through a few orthogonal linear combinations of features. This process is analogous to lossy compression, where if we use all the available dimensions, decompressing will fully recover the original data. That said, as we limit the number of linear combinations we use, we are able to capture almost the entire information with fewer data points, but unable to fully recover the original data after decompression. \n",
    "\n",
    "It is important to highlight that we are not simply selecting some features with weight 1 and others with weight 0, but we are creating a matrix that will project our original data $x$ onto a $k$-dimensional subspace where hopefully our data's important characteristics remain unchanged. We call this matrix $U_a$ and it is defined as the $k$-leftmost columns of the matrix $U$, where $k$ is our number of selected principal components. $k$ will also be the resulting dimension of our data after 'compression'. More detail follows:\n",
    "\n",
    "## Computation of PCA and Inverse PCA\n",
    "\n",
    "For the block matrix\n",
    "$$U = \\begin{bmatrix}\n",
    "\\overbrace{U_{a}}^{\\text{PC's}} & \\overbrace{U_{b}}^{\\text{excluded PC's}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "We can create a projection $P$ of our original vector $x$ onto our new subspace through the operation\n",
    "\n",
    "$$P=xU_a \\tag{1.1}$$\n",
    "\n",
    "Note that our transformation matrix $U \\in \\mathbb{R}^D$ is an orthonormal basis, and as such is unitary, ie. $UU^T=I_D$, the $D$-dimensional identity matrix. $U_a$ is not guaranteed to be unitary, but $U_aU_a^T$ should approach $I_D$ as more and more components are added. When all components are included, naturally, the equality is satisfied, since $U_a$ becomes $U$. The unitary property allows us to efficiently compute the inverse PCA transformation of our projected data $P$, through the operation:\n",
    "\n",
    "$$\\hat{x} = PU_a^T \\tag{1.2}$$\n",
    "\n",
    "Note that as $U_a\\rightarrow U$, $\\hat{x} = PU_a^T = xU_aU_a^T \\rightarrow xUU^T = x$ (See figure below)\n",
    "\n",
    "$\\hat{x}$ is our reconstructed data point, which under a well-suited PCA fit, will be close to $x$. We can measure this 'reconstruction error' through the mean squared error (MSE) of our reconstructed data.\n",
    "\n",
    "$$MSE=\\frac{\\sum_{i=0}^{N}(\\hat{x}_{i}-x_{i})^2}{N}$$\n",
    "\n",
    "\n",
    "## Mathematical Derivation\n",
    "It is fairly common knowledge that PCA can be achieved through the eigendecomposition of the feature correlation matrix, but it is less commonly known that this explained-variance-maximization is equivalent to the minimization of the MSE. The proof is as follows:\n",
    "\n",
    "Start with a vector $x$ and its reconstruction $\\text{PCA}(x) = P \\rightarrow \\text{Inverse PCA}(P) = \\hat{x}$. To minimize $MSE(\\hat{x})$ we setup the following unconstrained optimization:\n",
    "\n",
    "$$\\text{minimize} \\quad {\\frac{\\sum_{i=1}^{N}||\\hat{x_i}-x_i||^2}{N}}$$\n",
    "where $\\hat{x_i}$ equals the projection of $x_i$ onto the unit vector $U_a$ , multiplied by $U_a$. ie:\n",
    "$$\\hat{x_i} = (x_i \\cdot U_a) U_a$$\n",
    "Squared error for a single sample, therefore becomes,\n",
    "\n",
    "$$||\\hat{x_i}-x_i||^2 = {||(x_i \\cdot U_a) U_a - x_i||^2}\\\\\n",
    "= ((x_i \\cdot U_a) U_a - x_i)((x_i \\cdot U_a) U_a - x_i) \\\\\n",
    "= ((x_i \\cdot U_a) U_a)^2 \\underbrace{- x_i \\cdot (x_i \\cdot U_a) U_a - (x_i \\cdot U_a) U_a \\cdot x_i}_{\\text{rearrange dot products} \\rightarrow -2(x_i \\cdot U_a)(x_i \\cdot U_a)=-2(x_i \\cdot U_a)^2}  + \\underbrace{x_i\\cdot x_i}_{=||x_i||^2} \\\\\n",
    "= (x_i \\cdot U_a)^2 \\underbrace{U_a \\cdot U_a}_{||U_a||^2=1^2=1} - 2(x_i \\cdot U_a)^2 + ||x_i||^2\\\\\n",
    "= (x_i \\cdot U_a)^2 - 2(x_i \\cdot U_a)^2 + ||x_i||^2\\\\\n",
    "= ||x_i||^2 - (x_i \\cdot U_a)^2$$\n",
    "\n",
    "Over all terms, the mean squared error is then defined as\n",
    "\n",
    "$$\\frac{\\sum_{i=1}^{N}||x_i||^2 - (x_i \\cdot U_a)^2}{N} $$\n",
    "$$ = \\frac{\\sum_{i=1}^{N}||x_i||^2}{N} - \\frac{\\sum_{i=1}^{N}(x_i \\cdot U_a)^2}{N} \\tag{2.1}$$\n",
    "\n",
    "Note that the first term in eq $(2.1)$ is always going to be non-negative and is not going to depend on our choice of $U_a$, meaning that the problem \n",
    "$$\\text{minimize} \\quad {\\frac{\\sum_{i=1}^{N}||\\hat{x_i}-x_i||^2}{N}}$$\n",
    "is equivalent to maximizing the second term in eq. $(2.1)$.\n",
    "$$\\text{maximize} \\quad \\frac{\\sum_{i=1}^{N}(x_i \\cdot U_a)^2}{N} \\tag{2.2}$$\n",
    "Here we make note of the variance formula for a vector $v$. $Var(v)=E[v^2]-E[v]^2$. For our vector of projections, we have\n",
    "$P = \\textbf{x}U_a = \\begin{bmatrix} \n",
    "x_1 \\cdot U_a\\\\\n",
    "x_2 \\cdot U_a\\\\\n",
    "... \\\\\n",
    "x_N \\cdot U_a\\\\\n",
    "\\end{bmatrix} $ where\n",
    "$E[P] = \\begin{bmatrix} \n",
    "E[x_1 \\cdot U_a]\\\\\n",
    "E[x_2 \\cdot U_a]\\\\\n",
    "... \\\\\n",
    "E[x_N \\cdot U_a]\\\\\n",
    "\\end{bmatrix}  = \\begin{bmatrix} \n",
    "E[x_1] \\cdot U_a\\\\\n",
    "E[x_2] \\cdot U_a\\\\\n",
    "... \\\\\n",
    "E[x_N] \\cdot U_a\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "0\\\\\n",
    "0\\\\\n",
    "... \\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} $\n",
    "our variance is exactly equal to $E[P^2]$, which is exactly what eq. $(2.2)$ is maximizing.\n",
    "\n",
    "### Maximizing Variance/Minimizing MSE\n",
    "For a single vector $u$:\n",
    "$$\\text{maximize} \\quad \\sigma^2=\\frac{\\sum_{i=1}^{N}(x_i \\cdot u)^2}{N}$$\n",
    "$$\\textrm{s.t.} \\quad u^Tu=1$$\n",
    "Our constraint ensures $u$, our projection vector, is of unit length.\n",
    "We start by restating our cost function as $$\\sigma^2=\\frac{\\sum_{i=1}^{N}(x_i \\cdot u_i)^2}{N} = \\frac{(x_i u)^T(x_i u)}{N} = \\frac{u^Tx^Tx u}{N} \\tag{2.3}$$\n",
    "We can make use of the fact that our data vector $x$ is zero mean and that for a vector $y$, its correlation matrix $R_y$ $$ = E[(y-\\mu_y)(y-\\mu_y)^T] = E[yy^T]$$\n",
    "Taking $y=x^T$ \n",
    "$$R_{x^T} = E[x^Tx] = \\frac{x^Tx}{N}$$Thus, eq $(2.3)$ simplifies to\n",
    "$$u^TR_{x^T}u$$\n",
    "To optimize, we write the lagrangian as follows:\n",
    "$$\\mathbb{L} = u^TR_{x^T}u  - \\nu(uu^T-1)$$\n",
    "where <em>nu</em>, $\\nu$, is our lagrange multiplier for equality constraints. We differentiate with respect to $u$ and set to zero to find the optimum\n",
    "\n",
    "$$\\frac{\\partial\\mathbb{L}}{\\partial{u}}=\\frac{\\partial{u^TR_{x^T}u}}{\\partial{u}}-\\frac{\\partial{(\\nu u^Tu-\\nu)}}{\\partial{u}}=\\frac{2R_{x^T}u}{N}-2\\nu u=0$$\n",
    "\n",
    "$$\\therefore R_{x^T}\\textbf{u}=\\nu \\textbf{u}$$\n",
    "\n",
    "We know that for a scalar $\\lambda$, a matrix $A$ and vector $v$, if $A\\textbf{v}=\\lambda \\textbf{v}$ then $v$ is an eigenvector of $A$ with corresponding eigenvalue $\\lambda$. It is easy to see now that to maximize the variance in our optimization problem, we need to pick the eigenvector of $R_{x^T}$, $u$ with largest corresponding eigenvalue $\\nu$. Q.E.D.\n",
    "\n",
    "Helpful resources: \n",
    "- Boyd, Stephen, Stephen P. Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n",
    "- https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\augus\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\augus\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "C:\\Users\\augus\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "root = '~/Documents/AI/datasets/four-shapes/shapes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code should work on any set of png images of the form\n",
    "\n",
    "+-- root<br>\n",
    "|  +-- class1<br>\n",
    "|  +--  +-- sample_c1_1.png<br>\n",
    "|  +--  +--  sample_c1_2.png<br>\n",
    "|  +--  +--  ...<br>\n",
    "|  +--  class2<br>\n",
    "|  +--  +--   sample_c2_1.png<br>\n",
    "|  +--  +--   sample_c2_2.png<br>\n",
    "|  +--  +--   ...<br>\n",
    "| ...<br>\n",
    "|  +--  classN<br>\n",
    "|  +--  +--   sample_N2_1.png<br>\n",
    "|  +--  +--   sample_N2_2.png<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        root, \n",
    "        img_shape=200,\n",
    "        label_names={\n",
    "            'circle': 0,\n",
    "            'square': 1,\n",
    "            'star': 2,\n",
    "            'triangle': 3,\n",
    "        },\n",
    "        downsample = True,\n",
    "        downsample_granularity = 4\n",
    "    ):\n",
    "        if img_shape%downsample_granularity:\n",
    "            warnings.warn(\"Image shape is not divisible by downsample granularity. Sampling may be off-center\")\n",
    "        \n",
    "        self.root = root\n",
    "        self.fps = []\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        # center downsampling\n",
    "        downsampled_idxs = np.arange(0, img_shape, downsample_granularity)+downsample_granularity//2\n",
    "        self.downsample_grid = tuple(np.meshgrid(downsampled_idxs, downsampled_idxs))\n",
    "        \n",
    "        # store map of shapes\n",
    "        self.label_names = label_names\n",
    "        \n",
    "        # with the appropriate directory structure, this will read all the data filepaths\n",
    "        for label in self.label_names:\n",
    "            fps_of_class_i =  glob.glob(self.root + f'/{label}/**png')\n",
    "            self.fps += glob.glob(self.root + f'/{label}/**png')\n",
    "            # attach label to it\n",
    "            self.labels += [self.label_names[label]] * len(fps_of_class_i)\n",
    "        self.labels = np.array(self.labels)\n",
    "            \n",
    "    def downsample_img(self, img):\n",
    "        \"\"\"\n",
    "        Simple downsampling for computational gain\n",
    "        \"\"\"\n",
    "        return img[self.downsample_grid]\n",
    "            \n",
    "    def load_data(self):\n",
    "        # read shape from file\n",
    "        for fp in tqdm(self.fps):\n",
    "            # convert png to np array\n",
    "            img = np.asarray(\n",
    "                    Image.open(fp)\n",
    "                )\n",
    "            \n",
    "            if self.downsample:\n",
    "                img = self.downsample_img(img)\n",
    "                \n",
    "            self.data.append(\n",
    "                img.flatten()\n",
    "            )\n",
    "            \n",
    "        self.data = np.array(self.data)\n",
    "        # ensure equal # of labels and data\n",
    "        assert self.data.shape[0] == self.labels.shape[0]\n",
    "        \n",
    "\n",
    "    def sample(self, n=100):\n",
    "        \"\"\"\n",
    "        provide a size n random sample of data and labels from dataset\n",
    "        args:\n",
    "        - n (int): sample size\n",
    "        \"\"\"\n",
    "        indices = np.random.choice(len(self.data), n)\n",
    "        return self.data[indices], self.labels[indices]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        For easy indexing\n",
    "        \"\"\"\n",
    "            \n",
    "        return self.data[i], self.labels[i]\n",
    "    \n",
    "    def show(self, i=None, img=None):\n",
    "        \"\"\"\n",
    "        show image of sample\n",
    "        args:\n",
    "        - i (int): index in dataset\n",
    "        \"\"\"\n",
    "        if i is None and img is None:\n",
    "            i = np.random.choice(self.data.shape[0])\n",
    "        \n",
    "        if img is None:\n",
    "            img = self[i][0]\n",
    "        \n",
    "        square_img = img.reshape(int(np.sqrt(img.shape)), -1)\n",
    "        \n",
    "        Image.fromarray(square_img).show()\n",
    "        return \n",
    "        \n",
    "class PCA:\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        \"\"\"\n",
    "        eq 1.1 in description\n",
    "        Args:\n",
    "        - data (np.array): data to be projected P = x @ U_a\n",
    "        \"\"\"\n",
    "        return abs(np.matmul(data, self.U_a))\n",
    "    \n",
    "    def __inverse__(self, data):\n",
    "        \"\"\"\n",
    "        eq 1.2 in description. Invert the transformation and re-add the original data's mean\n",
    "        Args:\n",
    "        - data (np.array): P @ U_a^T = x @ U_a @ U_a^T projections to to be transformed\n",
    "        \"\"\"\n",
    "        return abs(np.matmul(data, self.U_a.T)) + self.data_mean\n",
    "    \n",
    "    def __init__(self, dataset, num_components=3, autofit=True):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        Args:\n",
    "        - dataset (np.array): dataset with which to calculate mean and fit\n",
    "        - num_components (int): number of selected principal components. 'k' in our derivation\n",
    "        - autofit (bool): construct and fit at same time\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_mean = sample.mean(axis=0, keepdims=True)\n",
    "        self.num_components = num_components\n",
    "        if autofit:\n",
    "            self.fit(dataset)\n",
    "        else:\n",
    "            warings.warn(\"PCA instantiated w/o having been fit. To fit, call <pca_object>.fit(dataset)\")\n",
    "        \n",
    "    def fit(self, dataset, verbose=True):\n",
    "        \"\"\"\n",
    "        Calculate covariance matrix R_{x^T}, and eigendecompose.\n",
    "        Original drawback, very computationally expensive if not optimized, ie doing manually like I am\n",
    "        MemoryError: Unable to allocate 23.8 GiB for an array with shape (40000, 40000) and data type complex128\n",
    "        We brute force solved this by downsampling.\n",
    "        Args:\n",
    "        - dataset (np.array): data to be de-meaned and fit\n",
    "        - verbose (bool): print stuff\n",
    "        \"\"\"\n",
    "        if verbose: print(\"Centering Data at 0 across all features\")\n",
    "        dataset = dataset - self.data_mean()\n",
    "        if verbose: print(\"Performing Eigendecomposition on Covariance Matrix\")\n",
    "        \n",
    "        # lambda eigenvalues, u eigenvectors\n",
    "        self.L, self.U = np.linalg.eig(np.cov(dataset))\n",
    "        # all eigenvectors should be unit length\n",
    "        if verbose: print(\"Norms of first and last eigenvector (should be ~ 1)\", np.matmul(U[:, 0].T, U[:, 0]), np.matmul(U[:, -1].T, U[:, -1]))\n",
    "        # k-leftmost columns of our eigenvector matrix\n",
    "        self.U_a = self.U[:, :self.num_components]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(root, downsample_granularity=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.sample(1600)[0]\n",
    "\n",
    "# make data zero-mean\n",
    "sample = sample - sample.mean(axis=0, keepdims=True)\n",
    "\n",
    "# transpose data because we're correlating features, not samples\n",
    "sample = sample.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda eigenvalues, u eigenvectors\n",
    "L, U = np.linalg.eig(np.cov(sample))\n",
    "# all eigenvectors should be unit length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(np.matmul(U[:, :], U[:, :].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(abs(np.cumsum(L)/np.sum(L)))\n",
    "plt.title(\"Explained variance per added eigenvector\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(layout=dict(title = 'PCA Transformed Data'))\n",
    "\n",
    "for key, val in dataset.label_names.items():\n",
    "    sub_dataset = dataset[dataset[:][1] == val]\n",
    "    pca_transformed = abs(np.matmul(sub_dataset[0], U[:, :3]) )\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x = pca_transformed[:, 0],\n",
    "        y = pca_transformed[:, 1],\n",
    "        z = pca_transformed[:, 2],\n",
    "        name = key,\n",
    "        mode='markers'\n",
    "    ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
